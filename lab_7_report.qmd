---
title: "lab_7_report"
format: html
editor: visual
---

# Modeling Gravier data with XGBOOST

We do a PCA following directions of Claus O. Wilke

## Download/import libraries including random forest

```{r}
install.packages("ranger")
```

```{r}
library(tidyverse)
library(broom)
library(parsnip)
library(yardstick)
```

```{r}
#| eval: true
# Setting up directories
data <- "data/"
raw_dir <- "data/_raw/"
data_file <- "gravier.RData"
data_loc <- "https://github.com/ramhiser/datamicroarray/raw/master/data/"

#Creating raw directory
if( !dir.exists(data) ){
  dir.create(path = data)
}

#Creating raw directory
if( !dir.exists(raw_dir) ){
  dir.create(path = raw_dir)
}

#Downloading datafile
if( !file.exists(str_c(raw_dir, data_file)) ){
  download.file(
    url = str_c(data_loc, data_file),
    destfile = str_c(raw_dir, data_file))
}
load(file = str_c(raw_dir, data_file))
```

```{r}
gravier
```

## Random forest

```{r}
#define the random forest object
rand_object <- rand_forest(
  mode = "classification",
  engine = "ranger",
  mtry = NULL,
  trees = 168,
  min_n = NULL
)
```

```{r}
#split into train and test
split <- floor(length(gravier$y) * 0.75)
train <- list(
  x = gravier$x[1:split, ],
  y = gravier$y[1:split]
)

test <- list(
  x = gravier$x[(split + 1):length(gravier$y), ],
  y = gravier$y[(split + 1):length(gravier$y)]
)
```

```{r}
#fit the random forest to the train data
rand_fit <- rand_object |> 
  set_engine('ranger') |> 
  fit_xy(x = train$x,
         y = train$y)
```

```{r}
#predict on the test data
test_predictions <- predict(rand_fit, test$x, type = "class")
#calculate accuracy
accuracy_vec(truth = test$y, estimate = test_predictions$.pred_class)
```

```{r}
# let's check the original distribution to see if our random forest did well
print(sum(test$y == "good")/36*100)
# That seems quite random
```

So our random forest model doesn't seem to be able to predict this data very well. That is probably because we have quite few samples

```{r}
#But let's see the distribution of the training data
print(sum(train$y == "good")/126*100)
```

So we see that there is quite a difference between the distribution of "good" vs "poor" labels between our training and testing. We should probably do some cross validation to make sure, but this might be for another time :)
